{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQBUZTuMogui"
      },
      "source": [
        "# <b><font size=\"+2\"> Optional cells (unhide by clicking the arrow to the left)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8LavWMVp3fMt"
      },
      "outputs": [],
      "source": [
        "#@markdown # Unzip wavs from Gdrive\n",
        "#@markdown ###**(run this cell after running step 3)**\n",
        "#@markdown ---\n",
        "#@markdown ### If you have a lot of wav files, then zip them all into one file localy on your system, then upload it and copy the path.\n",
        "#@markdown ---\n",
        "\n",
        "import os\n",
        "\n",
        "zip_file_path = \"\\\"/content/drive/MyDrive/wavs.zip\\\"\" #@param {type:\"string\"}\n",
        "\n",
        "if os.path.isdir(\"/content/TTS-TT2/wavs\"):\n",
        "  !unzip $zip_file_path -d /content/TTS-TT2/wavs\n",
        "else:\n",
        "  print(\"Failed: You must run the second step in the training first, before running this.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhZSeuWHoj6g"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GA7bN2PxrdO_"
      },
      "outputs": [],
      "source": [
        "#@title ## <b><font color=\"pink\" size=\"+2\"> Check GPU\n",
        "_, card = !nvidia-smi --query-gpu=gpu_name --format='csv'\n",
        "print(card)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "z6aq6vRzqEV9"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\"> Step:1 **Mount your Google Drive**\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "from os.path import exists\n",
        "if not exists('/content/drive'):\n",
        "    drive.mount('drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7TPdPew-YZVA"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\"> **Step:2** **Install Tacotron2 (w/ARPAbet)**\n",
        "import os\n",
        "%cd /content/\n",
        "if not os.path.isdir(\"/content/TTS-TT2/\"):\n",
        "  print(\"Cloning justinjohn0306/TTS-TT2\")\n",
        "  !git clone https://github.com/justinjohn0306/TTS-TT2.git\n",
        "  %cd /content/TTS-TT2/\n",
        "  !git submodule init\n",
        "  !git submodule update\n",
        "%cd /content/TTS-TT2/\n",
        "#NVIDIA's requirements\n",
        "#I believe Colab gives us PyTorch and TF by default so we don't need anything else\n",
        "#Versions specified in requirements.txt have conflicts so that's why we simply get current versions \n",
        "print(\"Downloading tacotron2 requirements\")\n",
        "!pip install matplotlib numpy inflect librosa==0.9.1 scipy Unidecode pillow\n",
        "#Our requirements\n",
        "#We'll need gdown to download some really cool things\n",
        "!pip install gdown==4.6.0\n",
        "#!pip install git+https://github.com/wkentaro/gdown.git\n",
        "import gdown\n",
        "!git submodule init\n",
        "!git submodule update\n",
        "!pip install -q unidecode tensorboardX\n",
        "!apt-get install pv\n",
        "!apt-get install jq\n",
        "!wget https://raw.githubusercontent.com/tonikelope/megadown/master/megadown -O megadown.sh\n",
        "!chmod 755 megadown.sh\n",
        "#Download NVIDIA's LJSpeech model\n",
        "tt2_pretrained = \"https://drive.google.com/uc?id=1c5ZTuT7J08wLUoVZ2KkUs_VdZuJ86ZqA\"\n",
        "if not os.path.isfile(\"/content/TTS-TT2/pretrained_model\"):\n",
        "  print(\"Downloading tt2 pretrained\")\n",
        "  gdown.download(tt2_pretrained, \"/content/TTS-TT2/pretrained_model\", quiet=False)\n",
        "if not os.path.isfile(\"/content/TTS-TT2/text/merged.dict.txt\"):\n",
        "  print(\"Applying cmudict patch\")\n",
        "  # !curl https://cdn.discordapp.com/attachments/820353681567907901/865742324084244480/tacotron2-cmudict-patch.zip -o /content/tacotron2-cmudict-patch.zip\n",
        "  gdown.download(\"https://drive.google.com/uc?id=1xgtiHABttD4MTds4KUPjghXfLqvzln2_\", \"/content/tacotron2-cmudict-patch.zip\", quiet=False)\n",
        "  !unzip -o /content/tacotron2-cmudict-patch.zip -d /content/TTS-TT2/text/\n",
        "  !mv /content/TTS-TT2/text/merged.dict.txt /content/TTS-TT2/\n",
        "\n",
        "latest_downloaded = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TFVNCB2BYh1Y"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\"> Step:3 **Apply additional ARPAbet patches**\n",
        "patches = [\n",
        "  (\"/content/TTS-TT2/train.patch\", \"\"\"17a18\n",
        "> from tqdm import tqdm\n",
        "18a20,26\n",
        "> from plotting_utils import plot_alignment_to_numpy\n",
        "> from IPython import display\n",
        "> from PIL import Image\n",
        "> \n",
        "> def plot_something_to_ipython(alignment):\n",
        ">   numpoop = plot_alignment_to_numpy(alignment)\n",
        ">   display.display(Image.fromarray(numpoop))\n",
        "28,29c36,37\n",
        "<     assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "<     print(\"Initializing Distributed\")\n",
        "---\n",
        ">     assert torch.cuda.is_available(), \"distributed mode requires CUDA\"\n",
        ">     print(\"initializing distributed\")\n",
        "39c47\n",
        "<     print(\"Done initializing distributed\")\n",
        "---\n",
        ">     print(\"done initializing distributed\")\n",
        "86c94\n",
        "<     print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "---\n",
        ">     print(\"warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "101c109\n",
        "<     print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "---\n",
        ">     print(\"loading checkpoint '{}'\".format(checkpoint_path))\n",
        "107c115\n",
        "<     print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "---\n",
        ">     print(\"loaded checkpoint '{}' from iteration {}\" .format(\n",
        "113c121\n",
        "<     print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "---\n",
        ">     print(\"saving model and optimizer state at iteration {} to {}\".format(\n",
        "145c153\n",
        "<         print(\"Validation loss {}: {:9f}  \".format(iteration, val_loss))\n",
        "---\n",
        ">         print(\"validation loss {}: {:9f}  \".format(iteration, val_loss))\n",
        "146a155,161\n",
        ">         \n",
        ">         # plot_something_to_ipython\n",
        ">         # https://github.com/NVIDIA/tacotron2/blob/master/logger.py\n",
        ">         _, _, _, alignments = y_pred\n",
        ">         from random import randint\n",
        ">         idx = randint(0, alignments.size(0)-1)\n",
        ">         plot_something_to_ipython(alignments[idx].data.cpu().numpy().T)\n",
        "169a185\n",
        ">     print(\"starting with {} learning rate\".format(learning_rate))\n",
        "207,208c223,225\n",
        "<         print(\"Epoch: {}\".format(epoch))\n",
        "<         for i, batch in enumerate(train_loader):\n",
        "---\n",
        ">         print(\"starting epoch {} at iteration {}\".format(epoch, iteration))\n",
        ">         epochstart = time.perf_counter()\n",
        ">         for i, batch in tqdm(enumerate(train_loader)):\n",
        "240,241c257,258\n",
        "<                 print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n",
        "<                     iteration, reduced_loss, grad_norm, duration))\n",
        "---\n",
        ">                 #print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n",
        ">                 #    iteration, reduced_loss, grad_norm, duration))\n",
        "243a261\n",
        ">             from random import random\n",
        "245,246c263,266\n",
        "<             if not is_overflow and (iteration % hparams.iters_per_checkpoint == 0):\n",
        "<                 validate(model, criterion, valset, iteration,\n",
        "---\n",
        ">             iteration += 1\n",
        ">         print(\"\\nepoch {} took {} seconds\".format(epoch, time.perf_counter() - epochstart))\n",
        ">         #what could possibly go wrong\n",
        ">         validate(model, criterion, valset, iteration,\n",
        "248a269\n",
        ">         if not is_overflow and (random() > 0.66): #(iteration % hparams.iters_per_checkpoint == 0):\n",
        "251,256c272,280\n",
        "<                         output_directory, \"checkpoint_{}\".format(iteration))\n",
        "<                     save_checkpoint(model, optimizer, learning_rate, iteration,\n",
        "<                                     checkpoint_path)\n",
        "< \n",
        "<             iteration += 1\n",
        "< \n",
        "---\n",
        ">                         output_directory, hparams.model_name)\n",
        ">                     try:\n",
        ">                         save_checkpoint(model, optimizer, learning_rate, iteration,\n",
        ">                                         checkpoint_path)\n",
        ">                     except KeyboardInterrupt:\n",
        ">                         print(\"you probably shouldnt ^C while im saving\")\n",
        ">                         save_checkpoint(model, optimizer, learning_rate, iteration,\n",
        ">                                         checkpoint_path)\n",
        ">                         print(\"ok it should be fine now\")\n",
        "\n",
        "\"\"\"),\n",
        "  (\"/content/TTS-TT2/plotting_utils.patch\", \"\"\"5c5\n",
        "< \n",
        "---\n",
        "> import io\n",
        "9,10c9,13\n",
        "<     data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n",
        "<     data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n",
        "---\n",
        ">     # no obvious way to make it rgba... https://github.com/matplotlib/matplotlib/issues/5336#issuecomment-388736185\n",
        ">     buf = io.BytesIO()\n",
        ">     fig.savefig(buf, format=\"rgba\")\n",
        ">     data = np.fromstring(buf.getvalue(), dtype=np.uint8, sep='')\n",
        ">     data = data.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
        "15c18\n",
        "<     fig, ax = plt.subplots(figsize=(6, 4))\n",
        "---\n",
        ">     fig, ax = plt.subplots(figsize=(9, 6))\n",
        "17c20\n",
        "<                    interpolation='none')\n",
        "---\n",
        ">                    interpolation='none', cmap='inferno')\n",
        "35c38\n",
        "<                    interpolation='none')\n",
        "---\n",
        ">                    interpolation='none', cmap='inferno')\n",
        "\n",
        "\"\"\")\n",
        "]\n",
        "for i, v in enumerate(patches):\n",
        "  to = v[0]\n",
        "  co = v[1]\n",
        "  with open(to, \"w\") as file:\n",
        "    file.write(co)\n",
        "\n",
        "from glob import glob \n",
        "for x in glob(\"*.patch\"):\n",
        "  base = x[:-6]\n",
        "  patch = x\n",
        "  py = base+\".py\"\n",
        "  !patch {py} {patch}\n",
        "  import time\n",
        "import argparse\n",
        "import math\n",
        "from numpy import finfo\n",
        "\n",
        "import torch\n",
        "from distributed import apply_gradient_allreduce\n",
        "import torch.distributed as dist\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from model import Tacotron2\n",
        "from data_utils import TextMelLoader, TextMelCollate\n",
        "from loss_function import Tacotron2Loss\n",
        "from logger import Tacotron2Logger\n",
        "from hparams import create_hparams\n",
        " \n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import layers\n",
        "from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "from text import text_to_sequence\n",
        "from math import e\n",
        "#from tqdm import tqdm # Terminal\n",
        "#from tqdm import tqdm_notebook as tqdm # Legacy Notebook TQDM\n",
        "from tqdm.notebook import tqdm # Modern Notebook TQDM\n",
        "from distutils.dir_util import copy_tree\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "def download_from_google_drive(file_id, file_name):\n",
        "  # download a file from the Google Drive link\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "\n",
        "def create_mels():\n",
        "    print(\"Generating Mels\")\n",
        "    stft = layers.TacotronSTFT(\n",
        "                hparams.filter_length, hparams.hop_length, hparams.win_length,\n",
        "                hparams.n_mel_channels, hparams.sampling_rate, hparams.mel_fmin,\n",
        "                hparams.mel_fmax)\n",
        "    def save_mel(filename):\n",
        "        audio, sampling_rate = load_wav_to_torch(filename)\n",
        "        if sampling_rate != stft.sampling_rate:\n",
        "            raise ValueError(\"{} {} SR doesn't match target {} SR\".format(filename, \n",
        "                sampling_rate, stft.sampling_rate))\n",
        "        audio_norm = audio / hparams.max_wav_value\n",
        "        audio_norm = audio_norm.unsqueeze(0)\n",
        "        audio_norm = torch.autograd.Variable(audio_norm, requires_grad=False)\n",
        "        melspec = stft.mel_spectrogram(audio_norm)\n",
        "        melspec = torch.squeeze(melspec, 0).cpu().numpy()\n",
        "        np.save(filename.replace('.wav', ''), melspec)\n",
        "\n",
        "    import glob\n",
        "    wavs = glob.glob('wavs/*.wav')\n",
        "    for i in tqdm(wavs):\n",
        "        save_mel(i)\n",
        "\n",
        "\n",
        "def reduce_tensor(tensor, n_gpus):\n",
        "    rt = tensor.clone()\n",
        "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
        "    rt /= n_gpus\n",
        "    return rt\n",
        "\n",
        "\n",
        "def init_distributed(hparams, n_gpus, rank, group_name):\n",
        "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
        "    print(\"Initializing Distributed\")\n",
        "\n",
        "    # Set cuda device so everything is done on the right GPU.\n",
        "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
        "\n",
        "    # Initialize distributed communication\n",
        "    dist.init_process_group(\n",
        "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
        "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
        "\n",
        "    print(\"Done initializing distributed\")\n",
        "\n",
        "\n",
        "def prepare_dataloaders(hparams):\n",
        "    # Get data, data loaders and collate function ready\n",
        "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
        "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
        "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        train_sampler = DistributedSampler(trainset)\n",
        "        shuffle = False\n",
        "    else:\n",
        "        train_sampler = None\n",
        "        shuffle = True\n",
        "\n",
        "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
        "                              sampler=train_sampler,\n",
        "                              batch_size=hparams.batch_size, pin_memory=False,\n",
        "                              drop_last=True, collate_fn=collate_fn)\n",
        "    return train_loader, valset, collate_fn\n",
        "\n",
        "\n",
        "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
        "    if rank == 0:\n",
        "        if not os.path.isdir(output_directory):\n",
        "            os.makedirs(output_directory)\n",
        "            os.chmod(output_directory, 0o775)\n",
        "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
        "    else:\n",
        "        logger = None\n",
        "    return logger\n",
        "\n",
        "\n",
        "def load_model(hparams):\n",
        "    model = Tacotron2(hparams).cuda()\n",
        "    if hparams.fp16_run:\n",
        "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model_dict = checkpoint_dict['state_dict']\n",
        "    if len(ignore_layers) > 0:\n",
        "        model_dict = {k: v for k, v in model_dict.items()\n",
        "                      if k not in ignore_layers}\n",
        "        dummy_dict = model.state_dict()\n",
        "        dummy_dict.update(model_dict)\n",
        "        model_dict = dummy_dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint_path, model, optimizer):\n",
        "    assert os.path.isfile(checkpoint_path)\n",
        "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
        "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
        "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
        "    learning_rate = checkpoint_dict['learning_rate']\n",
        "    iteration = checkpoint_dict['iteration']\n",
        "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
        "        checkpoint_path, iteration))\n",
        "    return model, optimizer, learning_rate, iteration\n",
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
        "    import random\n",
        "    if True:\n",
        "        print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
        "            iteration, filepath))\n",
        "        try:\n",
        "            torch.save({'iteration': iteration,\n",
        "                    'state_dict': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'learning_rate': learning_rate}, filepath)\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"interrupt received while saving, waiting for save to complete.\")\n",
        "            torch.save({'iteration': iteration,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'learning_rate': learning_rate}, filepath)\n",
        "        print(\"Model Saved\")\n",
        "\n",
        "def plot_alignment(alignment, info=None):\n",
        "    %matplotlib inline\n",
        "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
        "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
        "                   interpolation='none')\n",
        "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    xlabel = 'Decoder timestep'\n",
        "    if info is not None:\n",
        "        xlabel += '\\n\\n' + info\n",
        "    plt.xlabel(xlabel)\n",
        "    plt.ylabel('Encoder timestep')\n",
        "    plt.tight_layout()\n",
        "    fig.canvas.draw()\n",
        "    plt.show()\n",
        "\n",
        "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
        "             collate_fn, logger, distributed_run, rank, epoch, start_eposh, learning_rate):\n",
        "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
        "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
        "                                shuffle=False, batch_size=batch_size,\n",
        "                                pin_memory=False, collate_fn=collate_fn)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        for i, batch in enumerate(val_loader):\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            if distributed_run:\n",
        "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_val_loss = loss.item()\n",
        "            val_loss += reduced_val_loss\n",
        "        val_loss = val_loss / (i + 1)\n",
        "\n",
        "    model.train()\n",
        "    if rank == 0:\n",
        "        print(\"Epoch: {} Validation loss {}: {:9f}  Time: {:.1f}m LR: {:.6f}\".format(epoch, iteration, val_loss,(time.perf_counter()-start_eposh)/60, learning_rate))\n",
        "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
        "        if hparams.show_alignments:\n",
        "            %matplotlib inline\n",
        "            _, mel_outputs, gate_outputs, alignments = y_pred\n",
        "            idx = random.randint(0, alignments.size(0) - 1)\n",
        "            plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
        "\n",
        "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
        "          rank, group_name, hparams, log_directory2, save_interval, backup_interval): \n",
        "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
        "\n",
        "    Params\n",
        "    ------\n",
        "    output_directory (string): directory to save checkpoints\n",
        "    log_directory (string) directory to save tensorboard logs\n",
        "    checkpoint_path(string): checkpoint path\n",
        "    n_gpus (int): number of gpus\n",
        "    rank (int): rank of current gpu\n",
        "    hparams (object): comma separated list of \"name=value\" pairs.\n",
        "    \"\"\"\n",
        "    if hparams.distributed_run:\n",
        "        init_distributed(hparams, n_gpus, rank, group_name)\n",
        "\n",
        "    torch.manual_seed(hparams.seed)\n",
        "    torch.cuda.manual_seed(hparams.seed)\n",
        "\n",
        "    model = load_model(hparams)\n",
        "    learning_rate = hparams.learning_rate\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
        "                                 weight_decay=hparams.weight_decay)\n",
        "\n",
        "    if hparams.fp16_run:\n",
        "        from apex import amp\n",
        "        model, optimizer = amp.initialize(\n",
        "            model, optimizer, opt_level='O2')\n",
        "\n",
        "    if hparams.distributed_run:\n",
        "        model = apply_gradient_allreduce(model)\n",
        "\n",
        "    criterion = Tacotron2Loss()\n",
        "\n",
        "    logger = prepare_directories_and_logger(\n",
        "        output_directory, log_directory, rank)\n",
        "\n",
        "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
        "\n",
        "    # Load checkpoint if one exists\n",
        "    iteration = 0\n",
        "    epoch_offset = 0\n",
        "    if checkpoint_path is not None and os.path.isfile(checkpoint_path):\n",
        "        if warm_start:\n",
        "            model = warm_start_model(\n",
        "                checkpoint_path, model, hparams.ignore_layers)\n",
        "        else:\n",
        "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
        "                checkpoint_path, model, optimizer)\n",
        "            if hparams.use_saved_learning_rate:\n",
        "                learning_rate = _learning_rate\n",
        "            iteration += 1  # next iteration is iteration + 1\n",
        "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
        "    else:\n",
        "      os.path.isfile(\"/content/TTS-TT2/pretrained_model\")\n",
        "      %cd /dev/null\n",
        "      !/content/TTS-TT2/megadown.sh https://mega.nz/#!WXY3RILA!KyoGHtfB_sdhmLFoykG2lKWhh0GFdwMkk7OwAjpQHRo --o pretrained_model\n",
        "      %cd /content/TTS-TT2\n",
        "      model = warm_start_model(\"/content/TTS-TT2/pretrained_model\", model, hparams.ignore_layers)\n",
        "      # download LJSpeech pretrained model if no checkpoint already exists\n",
        "    \n",
        "    start_eposh = time.perf_counter()\n",
        "    learning_rate = 0.0\n",
        "    model.train()\n",
        "    is_overflow = False\n",
        "    # ================ MAIN TRAINNIG LOOP! ===================\n",
        "    for epoch in tqdm(range(epoch_offset, hparams.epochs)):\n",
        "        print(\"\\nStarting Epoch: {} Iteration: {}\".format(epoch, iteration))\n",
        "        start_eposh = time.perf_counter() # eposh is russian, not a typo\n",
        "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
        "            start = time.perf_counter()\n",
        "            if iteration < hparams.decay_start: learning_rate = hparams.A_\n",
        "            else: iteration_adjusted = iteration - hparams.decay_start; learning_rate = (hparams.A_*(e**(-iteration_adjusted/hparams.B_))) + hparams.C_\n",
        "            learning_rate = max(hparams.min_learning_rate, learning_rate) # output the largest number\n",
        "            for param_group in optimizer.param_groups:\n",
        "                param_group['lr'] = learning_rate\n",
        "\n",
        "            model.zero_grad()\n",
        "            x, y = model.parse_batch(batch)\n",
        "            y_pred = model(x)\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            if hparams.distributed_run:\n",
        "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
        "            else:\n",
        "                reduced_loss = loss.item()\n",
        "            if hparams.fp16_run:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if hparams.fp16_run:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
        "                is_overflow = math.isnan(grad_norm)\n",
        "            else:\n",
        "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
        "                    model.parameters(), hparams.grad_clip_thresh)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if not is_overflow and rank == 0:\n",
        "                duration = time.perf_counter() - start\n",
        "                logger.log_training(\n",
        "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
        "                #print(\"Batch {} loss {:.6f} Grad Norm {:.6f} Time {:.6f}\".format(iteration, reduced_loss, grad_norm, duration), end='\\r', flush=True)\n",
        "\n",
        "            iteration += 1\n",
        "        validate(model, criterion, valset, iteration,\n",
        "                 hparams.batch_size, n_gpus, collate_fn, logger,\n",
        "                 hparams.distributed_run, rank, epoch, start_eposh, learning_rate)\n",
        "        if (epoch+1) % save_interval == 0 or (epoch+1) == hparams.epochs: # not sure if the latter is necessary\n",
        "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path)\n",
        "        if backup_interval > 0 and (epoch+1) % backup_interval == 0:\n",
        "            save_checkpoint(model, optimizer, learning_rate, iteration, checkpoint_path + \"_epoch_%s\" % (epoch+1))\n",
        "        if log_directory2 != None:\n",
        "            copy_tree(log_directory, log_directory2)\n",
        "def check_dataset(hparams):\n",
        "    from utils import load_wav_to_torch, load_filepaths_and_text\n",
        "    import os\n",
        "    import numpy as np\n",
        "    def check_arr(filelist_arr):\n",
        "        for i, file in enumerate(filelist_arr):\n",
        "            if len(file) > 2:\n",
        "                print(\"|\".join(file), \"\\nhas multiple '|', this may not be an error.\")\n",
        "            if hparams.load_mel_from_disk and '.wav' in file[0]:\n",
        "                print(\"[WARNING]\", file[0], \" in filelist while expecting .npy .\")\n",
        "            else:\n",
        "                if not hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                    print(\"[WARNING]\", file[0], \" in filelist while expecting .wav .\")\n",
        "            if (not os.path.exists(file[0])):\n",
        "                print(\"|\".join(file), \"\\n[WARNING] does not exist.\")\n",
        "            if len(file[1]) < 3:\n",
        "                print(\"|\".join(file), \"\\n[info] has no/very little text.\")\n",
        "            if not ((file[1].strip())[-1] in r\"!?,.;:\"):\n",
        "                print(\"|\".join(file), \"\\n[info] has no ending punctuation.\")\n",
        "            mel_length = 1\n",
        "            if hparams.load_mel_from_disk and '.npy' in file[0]:\n",
        "                melspec = torch.from_numpy(np.load(file[0], allow_pickle=True))\n",
        "                mel_length = melspec.shape[1]\n",
        "            if mel_length == 0:\n",
        "                print(\"|\".join(file), \"\\n[WARNING] has 0 duration.\")\n",
        "    print(\"Checking Training Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.training_files) # get split lines from training_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Checking Validation Files\")\n",
        "    audiopaths_and_text = load_filepaths_and_text(hparams.validation_files) # get split lines from validation_files text file.\n",
        "    check_arr(audiopaths_and_text)\n",
        "    print(\"Finished Checking\")\n",
        "\n",
        "warm_start=False#sorry bout that\n",
        "n_gpus=1\n",
        "rank=0\n",
        "group_name=None\n",
        "\n",
        "# ---- DEFAULT PARAMETERS DEFINED HERE ----\n",
        "hparams = create_hparams()\n",
        "model_filename = 'current_model'\n",
        "hparams.training_files = \"filelists/clipper_train_filelist.txt\"\n",
        "hparams.validation_files = \"filelists/clipper_val_filelist.txt\"\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "hparams.decay_start = 15000\n",
        "hparams.A_ = 5e-4\n",
        "hparams.B_ = 8000\n",
        "hparams.C_ = 0\n",
        "hparams.min_learning_rate = 1e-5\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "hparams.batch_size = 32\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = []\n",
        "hparams.epochs = 10000\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "output_directory = '/content/drive/My Drive/colab/outdir' # Location to save Checkpoints\n",
        "log_directory = '/content/TTS-TT2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' filelists/*.txt\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "\n",
        "%cd /content/TTS-TT2\n",
        "\n",
        "data_path = 'wavs'\n",
        "!mkdir {data_path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kcfH75Dcw_uk"
      },
      "outputs": [],
      "source": [
        "#@markdown <b><font color=\"pink\"> Optional: \n",
        "\n",
        "#@markdown Revert all patches and reset \n",
        "#@markdown <b><font color=\"red\"> (DO NOT RUN)\n",
        "%cd /content/TTS-TT2/\n",
        "!git reset --hard HEAD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hQ3F8uZbTs8"
      },
      "source": [
        "### **Optional:** Run the cell below 👇 if you want to load another dataset and clear/remove the current one "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "54KO-O3PbGGf"
      },
      "outputs": [],
      "source": [
        "#@markdown <b><font color=\"pink\" size=\"+2\"> Clean tacotron2 dataset\n",
        "!rm -rf /content/TTS-TT2/wavs/*\n",
        "!rm -rf /content/TTS-TT2/filelists/*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OapsePIJBZq8"
      },
      "source": [
        "# **Training** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UP1trdpN_jV6"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\">  **Adjust model parameters**\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown #### Your desired model name:\n",
        "\n",
        "model_filename = 'test' #@param {type: \"string\"}\n",
        "\n",
        "#@markdown #### Upload your transcription / text to tacotron2/filelists and right click -> copy path:\n",
        "Training_file = \"filelists/list.txt\" #@param {type: \"string\"}\n",
        "hparams.training_files = Training_file\n",
        "hparams.validation_files = Training_file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# hparams to Tune\n",
        "#hparams.use_mmi=True,          # not used in this notebook\n",
        "#hparams.use_gaf=True,          # not used in this notebook\n",
        "#hparams.max_gaf=0.5,           # not used in this notebook\n",
        "#hparams.drop_frame_rate = 0.2  # not used in this notebook\n",
        "hparams.p_attention_dropout=0.1\n",
        "hparams.p_decoder_dropout=0.1\n",
        "\n",
        "# Learning Rate             # https://www.desmos.com/calculator/ptgcz4vzsw / http://boards.4channel.org/mlp/thread/34778298#p34789030\n",
        "hparams.decay_start = 15000         # wait till decay_start to start decaying learning rate\n",
        "\n",
        "#@markdown #### Lower learning rates will take more time but will lead to more accurate results:\n",
        "# Start/Max Learning Rate  \n",
        "hparams.A_ = 3e-4 #@param [\"3e-6\", \"1e-5\", \"1e-4\", \"5e-4\", \"1e-3\"] {type:\"raw\", allow-input: true}              \n",
        "hparams.B_ = 8000                   # Decay Rate\n",
        "hparams.C_ = 0                      # Shift learning rate equation by this value\n",
        "hparams.min_learning_rate = 1e-5    # Min Learning Rate\n",
        "\n",
        "# Quality of Life\n",
        "generate_mels = True\n",
        "hparams.show_alignments = True\n",
        "alignment_graph_height = 600\n",
        "alignment_graph_width = 1000\n",
        "\n",
        "#@markdown #### Your batch size, lower if you don't have enough ram:\n",
        "\n",
        "hparams.batch_size =  6#@param {type: \"integer\"}\n",
        "hparams.load_mel_from_disk = True\n",
        "hparams.ignore_layers = [] # Layers to reset (None by default, other than foreign languages this param can be ignored)\n",
        "use_cmudict = True #@param {type:\"boolean\"}\n",
        "#@markdown #### Your total epochs to train to. Not recommended to change:\n",
        "\n",
        "##@markdown #### Amount of epochs before stopping, preferably a very high amount to not stop.\n",
        "hparams.epochs =  250#@param {type: \"integer\"}\n",
        "\n",
        "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
        "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
        "\n",
        "#@markdown #### Where to save your model when training:\n",
        "output_directory = '/content/drive/MyDrive/colab/outdir' #@param {type: \"string\"}\n",
        "log_directory = '/content/TTS-TT2/logs' # Location to save Log files locally\n",
        "log_directory2 = '/content/drive/My Drive/colab/logs' # Location to copy log files (done at the end of each epoch to cut down on I/O)\n",
        "checkpoint_path = output_directory+(r'/')+model_filename\n",
        "\n",
        "##@markdown #### Train the model from scratch? (If yes, then uncheck the box below):\n",
        "#warm_start=True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "hparams.text_cleaners=[\"english_cleaners\"] + ([\"cmudict_cleaners\"] if use_cmudict is True else [])\n",
        "\n",
        "\n",
        "#@markdown Note:-\n",
        "\n",
        "#@markdown - The learning_rate value is ordered from smallest to largest, top to bottom.\n",
        "\n",
        "#@markdown - The smaller the \"learning rates\" value is, the longer it will take to train the model, but the more accurate the results will be.\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown Todo:-\n",
        "#@markdown - Disable warm_start\n",
        "#@markdown - Add tensorboard training monitor\n",
        "\n",
        "#@markdown ___\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "A2-xc6EcACWc"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\">  **Convert WAV files into Mel-Spectrograms (Run only once)**\n",
        "\n",
        "if generate_mels:\n",
        "    create_mels()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pDEiJHqJbLmY"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\"> **Check the working cmudict patch**\n",
        "%cd /content/TTS-TT2/\n",
        "import text\n",
        "print(text.sequence_to_text(text.text_to_sequence(\"We must capture an Earth creature, K 9, and return it back with us to Mars.\", [\"cmudict_cleaners\", \"english_cleaners\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "j_ATUV9uWHwx"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\"> **Check for missing files**\n",
        "\n",
        "# ---- Replace .wav with .npy in filelists ----\n",
        "!sed -i -- 's,.wav|,.npy|,g' {hparams.training_files}; sed -i -- 's,.wav|,.npy|,g' {hparams.validation_files}\n",
        "\n",
        "check_dataset(hparams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iqIk1dIuCqhx"
      },
      "outputs": [],
      "source": [
        "#@markdown ## <b><font color=\"pink\" size=\"+2\">  **Begin training**\n",
        "#@markdown ___\n",
        "#@markdown ### How often to save (number of epochs)\n",
        "#@markdown `10` by default. Raise this if you're hitting a rate limit. If you're using a particularly large dataset, you might want to set this to `1` to prevent loss of progress.\n",
        "save_interval =  10#@param {type: \"integer\"}\n",
        "#\n",
        "#@markdown ### How often to backup (number of epochs)\n",
        "#@markdown `-1` (disabled) by default. This will save extra copies of your model every so often, so you always have something to revert to if you train the model for too long. This *will* chew through your Google Drive storage.\n",
        "backup_interval =  -1#@param {type: \"integer\"}\n",
        "#\n",
        "\n",
        "print('FP16 Run:', hparams.fp16_run)\n",
        "print('Dynamic Loss Scaling:', hparams.dynamic_loss_scaling)\n",
        "print('Distributed Run:', hparams.distributed_run)\n",
        "print('cuDNN Enabled:', hparams.cudnn_enabled)\n",
        "print('cuDNN Benchmark:', hparams.cudnn_benchmark)\n",
        "train(output_directory, log_directory, checkpoint_path,\n",
        "      warm_start, n_gpus, rank, group_name, hparams, log_directory2,\n",
        "      save_interval, backup_interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4ql3OWWPKZv"
      },
      "source": [
        "JS to prevent idle timeout:\n",
        "\n",
        "Press F12 OR CTRL + SHIFT + I OR right click on this website -> inspect.\n",
        "Then click on the console tab and paste in the following code.\n",
        "\n",
        "```javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "iQBUZTuMogui"
      ],
      "name": "FakeYou_Tacotron_2_Training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
